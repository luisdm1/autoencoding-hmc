rm(list=ls())
library("bayesplot")
library("ggplot2")
library("GGally")
library("rstanarm")
library("MASS")
library("purrr")
library("tictoc")
library("mcmcse")
#####################################################################
#                       Simulate Data                               #
#####################################################################
set.seed(12345)
# simulate data
n = 1000
# number of parameters
m = 500
latent_dim = 50
hidden_dim = round((m+latent_dim)/2)
# hidden_dim = 100
nn_hidden_dim = 100
hidden_dim
hidden_dim = 100
var_X = 0.01
scale_intercept = 0.1
var_beta = 100
beta_min = -1
beta_max = 1
x1 = mvrnorm(n = n, mu = rep(0,m-1), Sigma = var_X*diag(m-1))
X = cbind(rep(scale_intercept,n), x1)
beta.T = runif(m, min = beta_min, max = beta_max)
cat('True parameters beta: ', beta.T)
prob.T = exp(X %*% beta.T)/(1+exp(X %*% beta.T))
y = numeric()
for(i in 1:n){y[i] = rbinom(1, size = 1, prob = prob.T[i])}
data = cbind(y,X)
#prior for beta
mu = rep(0, m)
sigma2 = rep(1, m)*var_beta
param = c(mu, sigma2)
U = function(beta, data, param){
y.U = data[,1]
X.U = data[,2:(m+1)]
mu.U = param[1:m]
sigma2.U = param[(m+1):(2*m)]
return(sum((beta-mu.U)^2/(2*sigma2.U)) - t(y.U)%*%(X.U%*%beta) + sum(log(1+exp(X.U%*%beta))))
}
grad_U = function(beta, data, param){
y.gU = data[,1]
X.gU = data[,2:(m+1)]
mu.gU = param[1:m]
sigma2.gU = param[(m+1):(2*m)]
return((beta -mu.gU)/sigma2.gU -t(X.gU)%*%y.gU + t(X.gU) %*% (exp(X.gU%*%beta)/(1+exp(X.gU%*%beta))))
}
#####################################################################
#                      Pretrain HMC samples                         #
#####################################################################
logistic_hmc_presample = function(start, data, param, L, epsilon, max.iter){
beta.vec = matrix(0, max.iter, m)
acc.vec = rep(0, max.iter)
acc.vec[1] = 0
beta.vec[1,] = start
current_beta = beta.vec[1,]
for (i in 2:max.iter){
result = hmc_lr_presample(U, grad_U, epsilon, L, current_beta, data, param)
beta.vec[i,] = result$sample
current_beta = beta.vec[i,]
acc.vec[i] = result$acc
}
return(list(samples = beta.vec, acc = acc.vec))
}
source("hmc_lr_presample.R")
presample_size = 1000
presample_results <- readRDS("data/presample_results.rds")
presample_accepted = presample_results$samples[presample_results$acc == 1,]
presample_accepted_size = dim(presample_accepted)[1]
cat("Acceptance ratio: ", sum(presample_results$acc)/(presample_size-1), '\n')
next_hmc_init = presample_results$samples[presample_size,]
pretrain_samples <- presample_results$samples
presample_train = presample_accepted[1:round(presample_accepted_size*0.9),]
presample_test = presample_accepted[(round(presample_accepted_size*0.9)+1):presample_accepted_size,]
mean_train = apply(presample_train,2,mean)
sd_train = apply(presample_train,2,sd)
norm_param = list(mean = mean_train, sd = sd_train)
#
x_train = t(apply(presample_train, 1, function(x)(x-norm_param$mean)/norm_param$sd))
x_test = t(apply(presample_test, 1, function(x)(x-norm_param$mean)/norm_param$sd))
# # no normalization
# x_train = presample_accepted[1:round(presample_accepted_size*0.9),]
# x_test = presample_accepted[(round(presample_accepted_size*0.9)+1):presample_accepted_size,]
# ##############################################
# Train the autoencoder with the pretrained samples
source("autoencoder_hmc.R")
tic("Training autoencoder")
autoencoder_results = autoencoder_hmc(x_train, x_test, model_savepath = "model_pretrain_autoencoder.hdf5", ncol_hidden_layer = hidden_dim, ncol_latent_layer = latent_dim, num_epoch = 1000, num_batchsize = 64, early.stop = TRUE, patience = 10)
toc()
loss_history = autoencoder_results$Hist
plot(loss_history)
model_pretrain_autoencoder = autoencoder_results$Model
encoder = autoencoder_results$Encoder
decoder = autoencoder_results$Decoder
library(keras)
library(tensorflow)
# Build tensors from the model
encoder_weights =get_weights(encoder)
U1 = encoder_weights[[1]]
d1 = encoder_weights[[2]]
U2 = encoder_weights[[3]]
decoder_weights =get_weights(decoder)
W1 = decoder_weights[[1]]
b1 = decoder_weights[[2]]
W2 = decoder_weights[[3]]
autoencoder_weights = list(U1 = U1, d1 = d1, U2 = U2, W1 = W1, b1 = b1, W2 = W2)
# ##############################################
# Calculate new gradient
# pre-calculate data and big matrix
y.gU = data[,1]
X.gU = data[,2:(m+1)]
XW2 = X.gU %*% t(W2)
sq_W2 = W2 %*% t(W2)
new_data = list(y.gU = y.gU, Z.gU = XW2, sq_W2 = sq_W2, W1 = W1, b1 = b1)
# gradient of latent U
gU_latent = function(beta_h, data, param){
y.gU = data$y.gU
Z.gU = data$Z.gU
sq_W2 = data$sq_W2
W1 = data$W1
b1 = data$b1
mu.gU = param[1:m]
sigma2.gU = param[(m+1):(2*m)]
tanh_term = tanh(as.vector(beta_h %*% W1+ t(b1)))
grad_1 = t(tanh_term%*%sq_W2) - t(Z.gU)%*%y.gU+ t(Z.gU) %*% (1/(1+exp(-Z.gU%*%tanh_term)))
v1 = 1-tanh_term^2
E1 = matrix(v1,nrow=dim(grad_1)[1],ncol=dim(grad_1)[2])
return((t(grad_1) * t(E1)) %*% t(W1))
}
# gradient of latent K
gK_latent = function(p_h, data){
sq_W2 = data$sq_W2
W1 = data$W1
b1 = data$b1
tanh_term = tanh(as.vector(p_h %*% W1+ t(b1)))
grad_1 = t(tanh_term%*%sq_W2)
v1 = 1-tanh_term^2
E1 = matrix(v1,nrow=dim(grad_1)[1],ncol=dim(grad_1)[2])
return((t(grad_1) * t(E1)) %*% t(W1))
}
## Implement HMC with autoencoder with correction
source("hmc_autoencoder_correction.R")
logistic_hmc_autoencoder = function(start, data, new_data, param, encoder,decoder, norm_param_autoencoder_input, autoencoder_weights, L, epsilon,max.iter){
acc_sum = 0
beta.vec = matrix(0, max.iter, m)
beta.vec[1,] = start
current_beta = beta.vec[1,]
for (i in 2:max.iter){
result = hmc_autoencoder_correction(U, gU_latent, gK_latent, encoder, decoder,norm_param_autoencoder_input, autoencoder_weights, epsilon, L, current_beta, data, new_data, param)
beta.vec[i,] = result$sample
current_beta = beta.vec[i,]
acc_sum = acc_sum + result$acc
}
cat("Acceptance ratio: ", acc_sum/(max.iter-1))
return(beta.vec)
}
tic("HMC with autoencoder sampling")
source("hmc_autoencoder_correction.R")
hmc_samples = logistic_hmc_autoencoder(start = next_hmc_init, data, new_data, param, encoder, decoder,norm_param_autoencoder_input = norm_param, autoencoder_weights, 10, 0.032, 2000)
toc()
# Plot and check sample autocorrelaiton
plot(hmc_samples[,1], type = 'l')
saveRDS(hmc_samples, "data/samples_auto_encoding_hmc_lr_correction.rds")
hmc_samples <- readRDS("data/samples_auto_encoding_hmc_lr_correction.rds")
